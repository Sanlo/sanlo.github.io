<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta use_mermaid="true" />
  <title>机器学习&amp;量化投资入门</title>
  <meta name="description" content="概要：介绍Logistic Regression的数学模型，推导并详细解释求解最优回归系数的过程 Python实现Logistic Regression的基本版 介绍sklearn中的Logistic Regression算法及其关键参数 实现一个基于Logistic Regression的简单选股策略" />

  <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon" />
  <link rel="icon" href="/favicon.ico?" type="image/x-icon" />
  <link rel="stylesheet" href="/css/fontawesome/css/font-awesome.min.css "> <link rel="stylesheet" href="/css/main.css ">
  <link rel="canonical" href="http://sanlo.github.io/2016/11/13/Introduction-to-quantitative-investment-and-machine-learning/" />
  <link rel="alternate" type="application/rss+xml" title="Sanlo Zhang" href="http://sanlo.github.io/feed.xml ">
</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">Sanlo Zhang</a>
        <small></small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                        <a href="/">
                    
                    <i class="fa fa-home"></i>Home
                    </a>
                </li>

                
                    
                    <li>
                        
                            <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>Archives
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                            <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>Categories
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                            <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>Tags
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                            <a href="/about/">
                        
                            <i class="fa fa-heart"></i>About
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>机器学习&量化投资入门</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2016-11-13
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
                


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#量化投资" title="Category: 量化投资" rel="category">量化投资</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

            <div class="label-card">
                
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0" title="Tag: 机器学习" rel="tag">机器学习</a-->
        <a href="/tag/#机器学习" title="Tag: 机器学习" rel="tag">机器学习</a>&nbsp;
    
        <!--a href="/tag/#%E9%87%8F%E5%8C%96%E6%8A%95%E8%B5%84" title="Tag: 量化投资" rel="tag">量化投资</a-->
        <a href="/tag/#量化投资" title="Tag: 量化投资" rel="tag">量化投资</a>&nbsp;
    
        <!--a href="/tag/#Python" title="Tag: Python" rel="tag">Python</a-->
        <a href="/tag/#Python" title="Tag: Python" rel="tag">Python</a>
    
  

</span>

            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
            <ul id="markdown-toc">
  <li><a href="#1-学习机器学习的动机" id="markdown-toc-1-学习机器学习的动机">1. 学习机器学习的动机</a></li>
  <li><a href="#2-什么是logistic-regression" id="markdown-toc-2-什么是logistic-regression">2. 什么是Logistic Regression</a>    <ul>
      <li><a href="#21-若有多个输入变量呢" id="markdown-toc-21-若有多个输入变量呢">2.1 若有多个输入变量呢？</a></li>
      <li><a href="#22-为什么称有如此形式的函数为对数几率函数呢" id="markdown-toc-22-为什么称有如此形式的函数为对数几率函数呢">2.2 为什么称有如此形式的函数为对数几率函数呢？</a></li>
      <li><a href="#23-如何求解回归模型中的最优回归系数权重呢" id="markdown-toc-23-如何求解回归模型中的最优回归系数权重呢">2.3 如何求解回归模型中的最优回归系数（权重）呢？</a>        <ul>
          <li><a href="#231-极大似然估计-是求解最优系数的常用方法之一" id="markdown-toc-231-极大似然估计-是求解最优系数的常用方法之一">2.3.1 极大似然估计 是求解最优系数的常用方法之一</a></li>
          <li><a href="#232-如何求解最优的回归系数w呢" id="markdown-toc-232-如何求解最优的回归系数w呢">2.3.2 如何求解最优的回归系数\(w\)呢？</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#3-python实现logistic-regression的基本版" id="markdown-toc-3-python实现logistic-regression的基本版">3. <code class="language-plaintext highlighter-rouge">Python</code>实现<code class="language-plaintext highlighter-rouge">Logistic Regression</code>的基本版</a>    <ul>
      <li><a href="#31-准备数据" id="markdown-toc-31-准备数据">3.1 准备数据：</a></li>
      <li><a href="#32-数据预处理包括" id="markdown-toc-32-数据预处理包括">3.2 数据预处理，包括：</a></li>
      <li><a href="#33-定义logistic函数" id="markdown-toc-33-定义logistic函数">3.3 定义<code class="language-plaintext highlighter-rouge">Logistic</code>函数：</a></li>
      <li><a href="#34-梯度上升法算法" id="markdown-toc-34-梯度上升法算法">3.4 梯度上升法算法：</a></li>
      <li><a href="#35-牛顿法算法" id="markdown-toc-35-牛顿法算法">3.5 牛顿法算法：</a></li>
      <li><a href="#36-选择最优算法根据求得的最优系数画出回归直线" id="markdown-toc-36-选择最优算法根据求得的最优系数画出回归直线">3.6 选择最优算法，根据求得的最优系数画出回归直线：</a></li>
      <li><a href="#37-下面一个cell运行整段代码" id="markdown-toc-37-下面一个cell运行整段代码">3.7 下面一个cell运行整段代码：</a></li>
    </ul>
  </li>
  <li><a href="#4-sklearn中的logistic-regression算法" id="markdown-toc-4-sklearn中的logistic-regression算法">4. sklearn中的<code class="language-plaintext highlighter-rouge">Logistic Regression</code>算法</a>    <ul>
      <li><a href="#41-实例化logistic-regression" id="markdown-toc-41-实例化logistic-regression">4.1 实例化Logistic Regression：</a></li>
      <li><a href="#42-训练数据" id="markdown-toc-42-训练数据">4.2 训练数据：</a></li>
      <li><a href="#43-输出模型系数输出测试数据的类别与概率" id="markdown-toc-43-输出模型系数输出测试数据的类别与概率">4.3 输出模型系数，输出测试数据的类别与概率：</a></li>
    </ul>
  </li>
  <li><a href="#5-基于logistic-regression的简单选股策略" id="markdown-toc-5-基于logistic-regression的简单选股策略">5. 基于<code class="language-plaintext highlighter-rouge">Logistic Regression</code>的简单选股策略</a></li>
</ul>

<style>
pre {
    overflow-x: auto;
    overflow-y: auto;
    max-height: 300px;
}
</style>

<h2 id="1-学习机器学习的动机">1. 学习机器学习的动机</h2>

<p><a href="https://link.zhihu.com/?target=http%3A//baike.baidu.com/link%3Furl%3Df7gg3CfSgV0FnVShzVofMK65MC3UU6kfUbK0_RupJ0ZHr4rG2UCfYWz5PEHjxqodrhf0O22RKKXJhi3NiCVZxa">詹姆斯·西蒙斯</a> 在TED的对话中有提到：<br />
Q：机器学习在这里扮演了怎样一个角色？<br />
A：某种意义上，我们做的就是机器学习。你观察一大堆数据，模拟不同的预测方案，直到你越来越擅长于此。我们所做之事，不见得一定有自我反馈，但确实有效。<br />
视频地址：<a href="http://v.qq.com/boke/page/s/0/w/s0186b0yrpw.html">与横扫华尔街数学家的珍贵对话</a></p>

<iframe frameborder="0" width="640" height="498" src="https://v.qq.com/iframe/player.html?vid=s0186b0yrpw" allowfullscreen=""></iframe>

<p>  然而现在有许多已实现的机器学习开源包可供我们调用，如sklearn，更高级的技术还有Hadoop、Spark等等。
那么是不是我们只须知道如何将训练数据与测试数据输入到模型，然后调用分类或回归的结果就好了呢？</p>

<p>　　理解其数学原理，自己再实现一遍算法有无必要呢？<br />
　　在汲取一些前辈的建议以及结合自己的思考后，我觉得自己亲手推一遍数学公式，再用Python实现算法还是有必要的。</p>

<ul>
  <li>因为这样不仅能使我们加深对机器学习本质的理解，还能让我们对模型与数据之间联系更加敏感。
例如，如何选择模型、如何选择特征、如何调整参数等等。</li>
  <li>其次，目前开源的机器学习算法包提供的都是通用型的算法，并非针对量化投资这一领域来进行优化。
所以，当有必要时，我们须根据自己的需求来优化这些通用算法，甚至重写。另一方面，真正有用的算法在量化领域是不会开源的。</li>
</ul>

<p>  当然机器学习这么多的算法也没有必要全都实现一遍，但常见的算法的核心部分还是要亲手实现的。</p>

<p>  <strong>笔记详细记录了Logistic Regression的数学原理与推导过程。在核心公式推导过程中，都会引出并详细解释关键的求解技巧和对应的数学知识。因此，只要是对本科高数还有记忆的同学都能完全理解和掌握。</strong></p>

<h2 id="2-什么是logistic-regression">2. 什么是Logistic Regression</h2>

<p>  首先，我们知道机器学习是一系列对数据执行分类、回归和聚类等操作的统计算法的统称，这些算法根据历史数据（训练数据）的特征，来对未来数据（测试数据）进行判断。文中介绍的是一种最常用也最重要的分类算法—Logistic Regression。<br />
　　首先我们引入对数几率函数（logistic function）：</p>

<!-- $$
\begin{equation}
y = \frac{1}{1+e^{-z}} 
\end{equation}\tag{1}
$$ -->

\[y = \frac{1}{1+e^{-z}} \tag{1}\]

<p>其函数曲线如下图所示：<br />
<img src="http://7u2ho6.com1.z0.glb.clouddn.com/jekyllWorkSpace.png" alt="" /></p>

<p>　　从上图我们可以看出，此函数可将定义域为(-∞, +∞)自变量z映射到(0,1)区间。若以 y=0.5为阈值，我们可以利用这个函数实现一个二分类器：</p>

<ul>
  <li>当 z&gt;0 ( y&gt;0.5)，将其划分为1类；</li>
  <li>当 z&lt;0 ( y&lt;0.5)，将其划分为0类。</li>
</ul>

<p>　　由于于输出y为0到1之间的连续值，我们也可以认为函数是对类别的概率估计。</p>

<h3 id="21-若有多个输入变量呢">2.1 若有多个输入变量呢？</h3>
<p>那么z由下列公式表示：</p>

\[z=w_0+w_1x_1+...+w_nx_n \tag{2}\]

<p>其中\(ω_i\)表示第i个输入变量的权重或是回归系数，第一个输入变量\(x_0\)默认为1。</p>

<p>　　公式(2)的向量形式又如下公式(3)表示：</p>

\[z=w^Tx\tag{3}\]

<p>其中，\(ω\)与\(x\)均为n行1列的列向量。</p>

<h3 id="22-为什么称有如此形式的函数为对数几率函数呢">2.2 为什么称有如此形式的函数为对数几率函数呢？</h3>

<p>通过简单的推导，<code class="language-plaintext highlighter-rouge">Logistic</code>函数可以写成如下形式：</p>

\[y=\frac{1}{1+e^-z}\Leftrightarrow ln\frac{y}{1-y}=w^Tx\tag{4}\]

<p>　其中\(ln\frac{y}{1−y}\)便称为<strong>对数几率</strong>（log odds）。“<strong>几率</strong>”（odds）反映了输入向量x被划分为1类的相对可能性。
　　因此公式（4）的意义是用线性回归模型去描述类别的<strong>对数几率</strong>。</p>

<h3 id="23-如何求解回归模型中的最优回归系数权重呢">2.3 如何求解回归模型中的最优回归系数（权重）呢？</h3>

<p>首先根据公式（4）我们可得：</p>

\[ln\frac{p(y=1|x)}{p(y=0|x)}=w^Tx\tag{5}\]

<p>进一步我们不难得到在输入为x的情况下，输出类别y分别为1和0的概率为：</p>

<p>\(\begin{align}
p(y=1|x)&amp;=\frac{e^{w^Tx}}{1+e^{w^Tx}}=\frac{1}{1+e^{-w^Tx}}\tag{6.1}\\
p(y=0|x)&amp;=\frac{1}{1+e^{w^Tx}}\tag{6.2}
\end{align}\)</p>
<h4 id="231-极大似然估计-是求解最优系数的常用方法之一">2.3.1 极大似然估计 是求解最优系数的常用方法之一</h4>

<p>极大似然估计的思想为：对于所有的抽样样本，使它们联合概率达到最大的系数便是统计模型最优的系数。<br />
　　因此对于第i个输入数据\(x_i\)，它被划分为\(y_i\)的概率可由公式（7）表示：</p>

\[p(y_i|x_i;w)=p(y_i=1|x_i;w)^{y_i}p(y_i=0|x_i;w)^{1-y_i}\tag{7}\]

<p>公式（7）巧妙之处在于，当\(y_i=1\)时，\(p(y_i=0|x_i;w)^{1-y_i}=1\)；而当\(y_i=0\)时，\(p(y_i=1|x_i;w)^{y_i}=1\)。<br />
在极大似然法中，我们假设样本之间都是独立同分布的，因此它们的联合概率就是它们各自概率的乘积。因此关于回归系数\(w\)的极大似然函数可由公式（8）表示：</p>

\[L(w)=\prod_{i=1}^{m}p(y_i=1|x_i;w)^{y_i}p(y_i=0|x_i;w)^{1-y_i}\tag{8}\]

<p>为了便于计算，我们对公式（8）两边同时取对数：</p>

\[lnL(w)=(\prod_{i=1}^{m}p(y_i=1|x_i;w)^{y_i}p(y_i=0|x_i;w)^{1-y_i})\tag{9}\]

<p>根据对数的性质，公式（9）可以逐步写成如下形式：</p>

<p>\(\begin{array}
lnL(w)=\left (\prod_{i=1}^{m}p(y_i=1|x_i;w)^{y_i}p(y_i=0|x_i;w)^{1-y_i}\right )\\
=\sum_{i=1}^{m}y_ilnp(y_i=1|x_i;w)+(1-y_i)lnp(y_i=0|x_i;w)\\
=\sum_{i=1}^{m}\left(y_iw^Tx-ln(1+e^{w^Tx}) \right)\tag{10}
\end{array}\\\)</p>
<h4 id="232-如何求解最优的回归系数w呢">2.3.2 如何求解最优的回归系数\(w\)呢？</h4>

<p>有两种经典的数值优化算法：a) 梯度上升法；b) 牛顿法。<br />
　　<strong>a) 梯度上升法</strong>：函数在某一点的梯度总是指向该函数增长最快的方向。因此沿着该函数的梯度方向探寻就能找到该函数的最大值。梯度上升法的迭代公式如下：</p>

\[w^{t+1}=w^t+\alpha\nabla_wlnL(w^t)\tag{11}\]

<p>公式（11）中\(\alpha\)表示每次迭代的步进。\(\nabla_w\)表示梯度算子。</p>

<p>根据以上定义，我们求取公式（10）的梯度：<br />
　　首先方程两边同时取微分：</p>

\[\begin{array}
d(lnL(w))=\sum_{i=1}^{m}\left(y_id\left(\boldsymbol{w}^T\boldsymbol{x}_i\right)-d\left(ln(1+e^{\boldsymbol{w}^T\boldsymbol{x}_i})\right)\right)\\
=\sum_{i=1}^{m}\left(y_id\left(\boldsymbol{w}^T\boldsymbol{x}_i\right)^T-\frac{e^{\boldsymbol{w}^T\boldsymbol{x}_i}}{1+e^{\boldsymbol{w}^T\boldsymbol{x}_i}}d\left(\boldsymbol{w}^T\boldsymbol{x}_i\right)^T\right)\\
=\sum_{i=1}^{m}y\boldsymbol{x}^T_id(\boldsymbol{w})-p(y_i=1|x_i;\boldsymbol{w})\boldsymbol{x}^T_id(\boldsymbol{w})\\
=\sum_{i=1}^{m}\left(\boldsymbol{x}^T_i(y_i-p(y_i=1|\boldsymbol{x}_i;\boldsymbol{w}))\right)d(\boldsymbol{w})
\end{array}\tag{12}\]

<p>因此公式（10）的梯度最终可写成公式（13）的形式：</p>

\[\nabla_wlinL(\boldsymbol{w})=\frac{\partial lnL(\boldsymbol{w})}{\partial\boldsymbol{w}}=\sum_{i=1}^{m}(y_i-p(y_i=1|\boldsymbol{x}_i;\boldsymbol{w}))\tag{13}\]

<p>从公式（12）到（13）有两处向量微分运算的小技巧（刚开始我也不会…）：<br />
　　第一处：公式（12）的第二行，由于\(\boldsymbol{w}^T\boldsymbol{x}_i\)为标量，我们取转置后，其性质不变。<br />
　　第二处：在向量的微分运算中，梯度与导数矩阵是相互装置的关系（这样说可能不是特别严谨…）。<br />
　　　　　　所以公式（12）的\(\boldsymbol{x}_i^T\)在公式（13）中变为\(\boldsymbol{x}_i\)。</p>

<p><strong>b) 牛顿法</strong>：其原理是利用泰勒公式不断迭代，从而逐次逼近零点或极值点。<br />
　　<strong>一阶展开求零点：</strong><br />
　　我们知道泰勒公式在x0处的一阶展开如下所示：</p>

\[f(x)\approx f(x_0)+(x-x_0)f'(x_0)\tag{14}\]

<p>我们用一阶展开式求解函数的零点：</p>

\[f(x_0)+(x-x_0)f'(x_0)=0\tag{15}\]

<p>稍作整理，公式（15）可写成如下形式：</p>

\[x=x_0-\frac{f(x_0)}{f'(x_0)}\tag{16}\]

<p>　　由于一阶展开式只是与原函数近似相等，因此公式（16）中的x并非函数的零点，而只是比x0更接近零点。因此通过对公式（16）迭代可以不断逼近函数零点。</p>

<p>　　<strong>如果是求函数的极值点呢？</strong>
　　那我们就要用到泰勒二阶展开式：</p>

\[f(x)\approx f(x_0)+(x-x_0)f'(x_0)+\frac{1}{2}(x-x_0)^2f''(x_0)=g(x)\tag{17}\]

<p>　　我们知道函数在其一阶导数等于0时取到极值，因此我们求上式(17) g′(x)=0的解：</p>

\[g'(x)=f'(x_0)+(x-x_0)f''(x_0)=0\tag{18}\]

<p>　　通过像公式（15）一样整理上式（18），可得：</p>

\[x=x_0-\frac{f'(x_0)}{f''(x_0)}\tag{19}\]

<p>　　<strong>如果输入是多维的变量呢？</strong><br />
　　高维情况的牛顿法迭代公式与公式（19）相似，其如下所示：</p>

\[\boldsymbol{x}^{t+1}=\boldsymbol{x}^t-\left[Hf(\boldsymbol{x}^t)\right]^{-1}\nabla f(\boldsymbol{x}^t)\tag{20}\]

<p>　　其中\(\nabla f(\boldsymbol{x}^t)\)表示的是函数\(f(\boldsymbol{x}^t)\)的梯度，\(\left[Hf(\boldsymbol{x}^t)\right]^{-1}\)表示的是函数的<code class="language-plaintext highlighter-rouge">Hessian</code>矩阵的逆矩阵。
　　当输入变量为一维时，<code class="language-plaintext highlighter-rouge">Hessian</code>矩阵其实可以理解为变量的二阶导数。其<code class="language-plaintext highlighter-rouge">Hessian</code>矩阵的定义如下：</p>

\[H(f(\boldsymbol{x}))=\frac{\partial^2f(\boldsymbol{x})}{\partial\boldsymbol{x}\partial{\boldsymbol{x}}^T}=
\begin{bmatrix}
 \frac{\partial^2f}{\partial x^2_1} &amp; \frac{\partial^2f}{\partial x_1\partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_1\partial x_n}\\ 
 \frac{\partial^2f}{\partial x_2\partial x_1} &amp; \frac{\partial^2f}{\partial x^2_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x_2\partial x_n}\\
 \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ 
 \frac{\partial^2f}{\partial x_n\partial x_1} &amp; \frac{\partial^2f}{\partial x_n\partial x_2} &amp; \cdots &amp; \frac{\partial^2f}{\partial x^2_n}\\
 \end{bmatrix}\tag{21}\]

<p>　　<strong>梯度上升法与牛顿法的比较：</strong><br />
　　梯度法是一阶收敛的，而牛顿法是二阶收敛的。因此牛顿法的迭代次数要少于梯度法，然而<code class="language-plaintext highlighter-rouge">Hessian</code>的逆矩阵的计算会增加算法的复杂度。<br />
　　这个问题又可以通过<code class="language-plaintext highlighter-rouge">Quasi-Newton</code>方法解决。</p>

<h2 id="3-python实现logistic-regression的基本版">3. <code class="language-plaintext highlighter-rouge">Python</code>实现<code class="language-plaintext highlighter-rouge">Logistic Regression</code>的基本版</h2>

<p>　　我们选择牛顿法求解<code class="language-plaintext highlighter-rouge">Logistic Regression</code>中的最优回归系数。<br />
　　根据牛顿法迭代公式（20）和Hessian矩阵的定义（21），本贴<code class="language-plaintext highlighter-rouge">Logistic Regression</code>中的回归系数迭代更新公式为：</p>

\[\boldsymbol{w}^{t+1}=\boldsymbol{w}^t-\left[\frac{\partial^2lnL(\boldsymbol{w})}{\partial\boldsymbol{w}\partial\boldsymbol{w}^T}\right]^{-1}\frac{\partial lnL(\boldsymbol{w})}{\partial\boldsymbol{w}}\tag{22}\]

<p>其中\(\frac{\partial lnL(w)}{\partial w}\)在公式（13）中已求得，而对\(\frac{\partial^2lnL(w)}{\partial w\partial w^T}\)的求解可按照上文所提到的两条向量微分运算方法来计算，因此：</p>

\[\frac{\partial^2lnL(\boldsymbol{w})}{\partial{\boldsymbol{w}}\partial{\boldsymbol{w}^T}}
=\sum_{i=1}^{m}\boldsymbol{x}_i\boldsymbol{x}_i^Tp(y_i=1|\boldsymbol{x}_i;\boldsymbol{w})\left(p(y_i=1|\boldsymbol{x}_i;\boldsymbol{w}-1\right)\tag{23}\]

<p>　好了！到此迭代公式已经完全掌握了，现在该实现算法了！</p>

<h3 id="31-准备数据">3.1 准备数据：</h3>

<p>　　从网上整理了一组数据: 前两列元素是<code class="language-plaintext highlighter-rouge">feature</code>；最后一列是<code class="language-plaintext highlighter-rouge">label</code>，其值是0或1。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">6.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.7</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.3</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.4</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.4</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.8</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.4</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.3</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.7</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.4</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.6</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.4</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.6</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span> 
</code></pre></div></div>

<h3 id="32-数据预处理包括">3.2 数据预处理，包括：</h3>
<p>　　1.增加常数项x0: 1.0；</p>

<p>　　2.feature 与 label的分离；</p>

<p>　　3.返回数组类型的feature 和 label。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># labels 用来标识每个数据的类别:0或1
</span>    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="c1"># train_X 输入数据列表: [[1.0, x_i1, x_i2],..., [1.0, x_n1, x_n2]]
</span>    <span class="n">train_X</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="c1"># 二维平面的直线方程为： ax + by + c = 0，因此我们需要再原来的输入数据中增加一常数项 1.0
</span>        <span class="n">train_X</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_X</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> 
</code></pre></div></div>

<h3 id="33-定义logistic函数">3.3 定义<code class="language-plaintext highlighter-rouge">Logistic</code>函数：</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="s">'''
        logistic function 即：
        牛顿迭代公式中的p(yi=1 | xi; w)
    '''</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="34-梯度上升法算法">3.4 梯度上升法算法：</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">):</span>
    <span class="c1"># 下面两行代码用于矩阵化 训练数据
</span>    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span> <span class="c1"># 100 行 3列
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span> <span class="c1"># 100 行 1列
</span>    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># 初始回归系数 weights 为全1的向量, weights的行数与输入的训练数据维度相同
</span>    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">columns</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># 步进alpha: 0.001
</span>    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="c1"># t 表示当前迭代的次数
</span>    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="n">P1</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
        <span class="n">weights</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">P1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">weights</span> 
</code></pre></div></div>
<p>  我们还是给出了梯度上升法的算法实现。 <br />
　　上述算法的倒数第三行，我们用矩阵形式表示梯度上升法的迭代公式，其具体数学形式如下：</p>

\[\begin{bmatrix}
w_0\\w_1\\w_2
\end{bmatrix}^{(t+1)}
=\begin{bmatrix}
w_0\\w_1\\w_2
\end{bmatrix}^t+\alpha
\begin{bmatrix}
1.0 &amp; x_{11} &amp; x_{12}\\
1.0 &amp; x_{21} &amp; x_{22}\\
\vdots &amp; \vdots &amp; \vdots\\
1.0 &amp; x_{n1} &amp; x_{n2}
\end{bmatrix}^T
\begin{bmatrix}
\begin{pmatrix}
y_1\\y_2\\\vdots\\y_n
\end{pmatrix}-
\begin{pmatrix}
p_1(\boldsymbol{x}_1;\boldsymbol{w})\\
p_1(\boldsymbol{x}_2;\boldsymbol{w})\\
\vdots\\
p_1(\boldsymbol{x}_n;\boldsymbol{w})
\end{pmatrix}
\end{bmatrix}\]

<h3 id="35-牛顿法算法">3.5 牛顿法算法：</h3>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">newton</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># 初始回归系数 weights 为全0的向量, weights的行数与输入的训练数据维度相同。
</span>    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">columns</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># t 表示当前迭代的次数
</span>    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="n">P1</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
        <span class="n">gradient</span> <span class="o">=</span>  <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">P1</span><span class="p">))</span>
        <span class="n">P_mat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">P1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
        <span class="n">hessian</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">P_mat</span> <span class="o">*</span> <span class="n">X</span>
        <span class="c1"># np.linalg.inv() 是求的Hessian矩阵的逆矩阵
</span>        <span class="n">weights</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">hessian</span><span class="p">)</span> <span class="o">*</span> <span class="n">gradient</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="n">weights</span> 
</code></pre></div></div>
<p>  与上小节3.4一致，我们都用矩阵形式表示迭代公式。其中梯度的具体数学形式在上小节已给出。<br />
　　在本节的牛顿算法中，Hessian矩阵具体数学形式如下：</p>

\[\boldsymbol{Hessian}=
\begin{bmatrix}
1.0 &amp; x_{11} &amp; x_{12}\\
1.0 &amp; x_{21} &amp; x_{22}\\
\vdots &amp; \vdots &amp; \vdots\\
1.0 &amp; x_{n1} &amp; x_{n2}
\end{bmatrix}^T
\begin{bmatrix}
p_1(\boldsymbol{x}_1;\boldsymbol{w})(p_1(\boldsymbol{x}_1;\boldsymbol{w})-1) &amp; &amp;\\
 &amp; p_1(\boldsymbol{x}_1;\boldsymbol{w})(p_1(\boldsymbol{x}_1;\boldsymbol{w})-1) &amp;\\
 &amp; \ddots &amp;\\
 &amp; &amp; p_1(\boldsymbol{x}_1;\boldsymbol{w})(p_1(\boldsymbol{x}_1;\boldsymbol{w})-1)\\
\end{bmatrix}
\begin{bmatrix}
1.0 &amp; x_{11} &amp; x_{12}\\
1.0 &amp; x_{21} &amp; x_{22}\\
\vdots &amp; \vdots &amp; \vdots\\
1.0 &amp; x_{n1} &amp; x_{n2}
\end{bmatrix}\]

<h3 id="36-选择最优算法根据求得的最优系数画出回归直线">3.6 选择最优算法，根据求得的最优系数画出回归直线：</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 牛顿法
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">newton</span><span class="p">(</span><span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># 梯度法
#weights = grad(preprocess(data)[0], preprocess(data)[1])
# 根据数据的类别，得到两个数据集
</span><span class="n">x0</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="n">y0</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="n">x1</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="n">y1</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">i</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x0</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y0</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># c=sns.color_palette()[1] ： seaborn 包中的配色方案
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'^'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># 画出回归直线： w0 + w1*x1 + w2*x2 = 0, 我们定义x1为横轴坐标，x2为纵轴坐标
</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x1'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'x2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span> 
</code></pre></div></div>

<h3 id="37-下面一个cell运行整段代码">3.7 下面一个cell运行整段代码：</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> <span class="c1">#数值计算包
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span> <span class="c1">#绘图包
</span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span> <span class="c1"># 绘图美化包 用于配色
</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">6.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">4.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.7</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.3</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.3</span><span class="p">,</span> <span class="mf">3.7</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.3</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.4</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.4</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.8</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.9</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.5</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.4</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.4</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.3</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.6</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.8</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.7</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">2.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.5</span><span class="p">,</span> <span class="mf">2.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.4</span><span class="p">,</span> <span class="mf">3.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.6</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.4</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.3</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.4</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.2</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.1</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">4.2</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.6</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.8</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.7</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.7</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.1</span><span class="p">,</span> <span class="mf">2.9</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.0</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">6.6</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.6</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">5.2</span><span class="p">,</span> <span class="mf">2.7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]]</span>

<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="c1"># labels 用来标识每个数据的类别:0或1
</span>    <span class="n">labels</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="c1"># train_X 输入数据列表: [[1.0, x_i1, x_i2],..., [1.0, x_n1, x_n2]]
</span>    <span class="n">train_X</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="c1"># 二维平面的直线方程为： ax + by + c = 0，因此我们需要再原来的输入数据中增加一常数项 1.0
</span>        <span class="n">train_X</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">labels</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_X</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">logit</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="s">'''
        logistic function 即：
        牛顿迭代公式中的p(yi=1 | xi; w)
    '''</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
    
<span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">500</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span> <span class="c1"># 100 行 3列
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span> <span class="c1"># 100 行 1列
</span>    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># 初始回归系数 weights 为全1的向量, weights的行数与输入的训练数据维度相同
</span>    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">columns</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># 步进alpha: 0.001
</span>    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.001</span>
    <span class="c1"># t 表示当前迭代的次数
</span>    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="n">P1</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
        <span class="n">weights</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">P1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="k">return</span> <span class="n">weights</span>

<span class="k">def</span> <span class="nf">newton</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">train_X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mat</span><span class="p">(</span><span class="n">labels</span><span class="p">).</span><span class="n">transpose</span><span class="p">()</span>
    <span class="n">rows</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># 初始回归系数 weights 为全0的向量, weights的行数与输入的训练数据维度相同。
</span>    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">columns</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># t 表示当前迭代的次数
</span>    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">max_iter</span><span class="p">:</span>
        <span class="n">P1</span> <span class="o">=</span> <span class="n">logit</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">weights</span><span class="p">))</span>
        <span class="n">gradient</span> <span class="o">=</span>  <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">Y</span> <span class="o">-</span> <span class="n">P1</span><span class="p">))</span>
        <span class="n">P_mat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">P1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">P1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
        <span class="n">hessian</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">T</span> <span class="o">*</span> <span class="n">P_mat</span> <span class="o">*</span> <span class="n">X</span>
        <span class="n">weights</span> <span class="o">-=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="n">inv</span><span class="p">(</span><span class="n">hessian</span><span class="p">)</span> <span class="o">*</span> <span class="n">gradient</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="k">return</span> <span class="n">weights</span>
    
<span class="c1"># 牛顿法
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">newton</span><span class="p">(</span><span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">data</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
<span class="c1"># 梯度法
# weights = grad(preprocess(data)[0], preprocess(data)[1])
# 根据数据的类别，得到两个数据集
</span><span class="n">x0</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="n">y0</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="n">x1</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="n">y1</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">i</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">x0</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y0</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">x1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># c=sns.color_palette()[1] ： seaborn 包中的配色方案
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'^'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">])</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">7.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># 画出回归直线： w0 + w1*x1 + w2*x2 = 0, 我们定义x1为横轴坐标，x2为纵轴坐标
</span><span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="n">weights</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'x1'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'x2'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span> 
</code></pre></div></div>
<p><img src="https://s2.ax1x.com/2020/02/29/3yxYiF.png" alt="" /></p>

<h2 id="4-sklearn中的logistic-regression算法">4. sklearn中的<code class="language-plaintext highlighter-rouge">Logistic Regression</code>算法</h2>
<p>  <code class="language-plaintext highlighter-rouge">sklearn</code>是<code class="language-plaintext highlighter-rouge">Python</code>实现的开源机器学习算法包，优矿正好也支持它。下面简单介绍下<code class="language-plaintext highlighter-rouge">sklearn</code>中<code class="language-plaintext highlighter-rouge">Logistic Regression</code>的使用。</p>

<h3 id="41-实例化logistic-regression">4.1 实例化Logistic Regression：</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="c1"># 参数C为正则化强度的倒数，用于控制回归系数的复杂度。
</span><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> 
</code></pre></div></div>
<p>  理解什么是正则化，我们先理解正则化的作用： 正则化可防止模型过于复杂。<br />
　　我们知道机器学习算法本质是一个最优化问题，算法根据训练数据所求得的模型系数可以使得数据分类、拟合等准确率更高。<br />
　　在保证模型准确性的同时，我们同样希望模型不过于复杂，因此我们就要在两者之间权衡。<br />
　　模型的准确性一般用代价函数来衡量，而模型的复杂度一般用系数的平方表示（系数向量的内积）： 
\(L'(\boldsymbol{w})=L(\boldsymbol{w})+C\boldsymbol{w}^T\boldsymbol{w}\) 
  在本问题中，我们将极大似然函数的负值视为代价函数，代价函数越小，说明极大似然函数越大，说明模型分类的准确性越高；<br />
  \(C\boldsymbol{w}^T\boldsymbol{w}\)用以衡量模型复杂度，C用以控制原代价函数与模型复杂度之间的折中。<br />
  <strong>很显然，C越大，算法对模型复杂度越敏感。</strong></p>

<h3 id="42-训练数据">4.2 训练数据：</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> 
</code></pre></div></div>
<p>  其中，<code class="language-plaintext highlighter-rouge">train_X</code>表示的训练数据的特征，其格式n行m列的矩阵，n为训练数据的个数，m为数据的特征个数（维度）；<br />
  <code class="language-plaintext highlighter-rouge">labels</code>表示的训练数据的类别，格式为n行1列的列向量。</p>

<h3 id="43-输出模型系数输出测试数据的类别与概率">4.3 输出模型系数，输出测试数据的类别与概率：</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 回归系数
</span><span class="k">print</span> <span class="s">'输出模型回归系数：'</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">coef_</span>
<span class="k">print</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span>

<span class="c1"># 预测类别
</span><span class="k">print</span> <span class="s">'输入数据[1.0, 5.0, 2.0]的类别：'</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="k">print</span> <span class="s">'输入数据[1.0, 5.0, 4.0]的类别：'</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="k">print</span> <span class="s">'</span><span class="se">\n</span><span class="s">'</span>

<span class="c1"># 预测概率
</span><span class="k">print</span>  <span class="s">'输入数据[1.0, 5.0, 2.0]为0类1类的概率：'</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="k">print</span>  <span class="s">'输入数据[1.0, 5.0, 4.0]为0类1类的概率：'</span><span class="p">,</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span> 
</code></pre></div></div>
<p>　　如果用上一节准备好数据，我们可分别得到如下结果：<br />
<strong>回归系数</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">输出模型回归系数</span><span class="err">：</span> <span class="p">[[</span><span class="o">-</span><span class="mf">0.55312625</span>  <span class="mf">2.2765652</span>  <span class="o">-</span><span class="mf">3.63240141</span><span class="p">]]</span> 
</code></pre></div></div>
<p><strong>预测类别</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">输入数据</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span><span class="n">的类别</span><span class="err">：</span> <span class="p">[</span> <span class="mf">1.</span><span class="p">]</span>
<span class="n">输入数据</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span><span class="n">的类别</span><span class="err">：</span> <span class="p">[</span> <span class="mf">0.</span><span class="p">]</span> 
</code></pre></div></div>
<p><strong>预测概率</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">输入数据</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]</span><span class="n">为0类1类的概率</span><span class="err">：</span> <span class="p">[[</span> <span class="mf">0.04689694</span>  <span class="mf">0.95310306</span><span class="p">]]</span>
<span class="n">输入数据</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]</span><span class="n">为0类1类的概率</span><span class="err">：</span> <span class="p">[[</span> <span class="mf">0.98597835</span>  <span class="mf">0.01402165</span><span class="p">]]</span> 
</code></pre></div></div>

<h2 id="5-基于logistic-regression的简单选股策略">5. 基于<code class="language-plaintext highlighter-rouge">Logistic Regression</code>的简单选股策略</h2>
<p>  该策略原理简单，即用<code class="language-plaintext highlighter-rouge">T-1</code>天股票的因子值，预测10个交易日后股票的涨跌。<br />
　　训练数据为<code class="language-plaintext highlighter-rouge">T-1</code>到<code class="language-plaintext highlighter-rouge">T-60</code>之间的滚动时间窗口。<br />
　　策略示意图如下所示：</p>

\[\underbrace{ {T-60,\cdots,}\overbrace{\overbrace{T-11}^{取特征因子},\cdots,\overbrace{T-1}^{判断类别：1涨0跌}T}^{一组训练数据}}_{所有训练数据}\]

<p>  在回测时，为了避免幸存者偏差，选股都是使用<code class="language-plaintext highlighter-rouge">set_universe('HS300', begin_data)</code>，没有直接使用<code class="language-plaintext highlighter-rouge">account.universe</code>。<br />
　　下面的cell给出策略代码及回测结果：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">CAL.PyCAL</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span><span class="p">,</span> <span class="n">timedelta</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">start</span> <span class="o">=</span> <span class="s">'2012-08-01'</span>                       <span class="c1"># 回测起始时间
</span><span class="n">end</span> <span class="o">=</span> <span class="s">'2015-08-01'</span>                         <span class="c1"># 回测结束时间
</span><span class="n">benchmark</span> <span class="o">=</span> <span class="s">'HS300'</span>                        <span class="c1"># 策略参考标准
</span><span class="n">universe</span> <span class="o">=</span> <span class="n">set_universe</span><span class="p">(</span><span class="s">'HS300'</span><span class="p">)</span>           <span class="c1"># 证券池，支持股票和基金
</span><span class="n">capital_base</span> <span class="o">=</span> <span class="mi">1000000</span>                     <span class="c1"># 起始资金
</span><span class="n">freq</span> <span class="o">=</span> <span class="s">'d'</span>                                 <span class="c1"># 策略类型，'d'表示日间策略使用日线回测，'m'表示日内策略使用分钟线回测
</span><span class="n">refresh_rate</span> <span class="o">=</span> <span class="mi">10</span>                           <span class="c1"># 调仓频率，表示执行handle_data的时间间隔，若freq = 'd'时间间隔的单位为交易日，若freq = 'm'时间间隔为分钟
</span>
<span class="c1"># 佣金万八+印花税千一；滑点默认
</span><span class="n">commission</span> <span class="o">=</span> <span class="n">Commission</span><span class="p">(</span><span class="n">buycost</span><span class="o">=</span><span class="mf">0.0008</span><span class="p">,</span> <span class="n">sellcost</span><span class="o">=</span><span class="mf">0.0018</span><span class="p">)</span> 
<span class="n">slippage</span> <span class="o">=</span> <span class="n">Slippage</span><span class="p">()</span> 

<span class="c1"># 选取的因子
</span><span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s">"LFLO"</span><span class="p">,</span><span class="s">"PB"</span><span class="p">,</span><span class="s">"PE"</span><span class="p">,</span><span class="s">"VOL10"</span><span class="p">,</span><span class="s">"VOL20"</span><span class="p">,</span><span class="s">"MA10"</span><span class="p">,</span><span class="s">"MA20"</span><span class="p">,</span> <span class="s">"RSI"</span><span class="p">,</span> <span class="s">"Volatility"</span><span class="p">]</span>
<span class="c1"># 股票数目
</span><span class="n">stocksNumber</span> <span class="o">=</span> <span class="mi">20</span>
<span class="c1"># 训练数据窗口长度：windows
</span><span class="n">windows</span> <span class="o">=</span> <span class="mi">60</span>
<span class="c1"># 预测天数： span
</span><span class="n">span</span> <span class="o">=</span> <span class="mi">10</span>


<span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">account</span><span class="p">):</span>                   <span class="c1"># 初始化虚拟账户状态
</span>    <span class="n">account</span><span class="p">.</span><span class="n">my_universe</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">handle_data</span><span class="p">(</span><span class="n">account</span><span class="p">):</span>                  <span class="c1"># 每个交易日的买入卖出指令
</span>    <span class="c1"># 构造训练数据：
</span>        <span class="c1"># 前 windows 个交易日内的每一个交易日，我们做如下判别：
</span>            <span class="c1"># 1. 如果某只股票的近10日超额收益率（相对HS300）大于0， 我们判别为1类，否则为0类；
</span>            <span class="c1"># 2. 该交易日10天前的股票因子值作为Logistic Regression的训练数据的特征值。
</span>            
    <span class="c1"># 创建一个空DataFrame来记录每个交易日的训练数据：train_X
</span>    <span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">()</span>
    
    <span class="n">yesterday</span> <span class="o">=</span> <span class="n">Date</span><span class="p">.</span><span class="n">fromDateTime</span><span class="p">(</span><span class="n">account</span><span class="p">.</span><span class="n">previous_date</span><span class="p">)</span>
    <span class="c1"># 从昨天开始，构造训练数据，直到第windows天前
</span>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">windows</span><span class="p">):</span>
        <span class="n">end_date</span> <span class="o">=</span> <span class="n">Calendar</span><span class="p">(</span><span class="s">'China.SSE'</span><span class="p">).</span><span class="n">advanceDate</span><span class="p">(</span><span class="n">yesterday</span><span class="p">,</span> <span class="n">Period</span><span class="p">(</span><span class="s">'-%dB'</span> <span class="o">%</span> <span class="n">t</span><span class="p">))</span>
        <span class="n">end_date</span> <span class="o">=</span> <span class="n">end_date</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y%m%d'</span><span class="p">)</span>
        <span class="c1"># span天前的日期为开始的日日期： bgn_data
</span>        <span class="n">bgn_date</span> <span class="o">=</span> <span class="n">Calendar</span><span class="p">(</span><span class="s">'China.SSE'</span><span class="p">).</span><span class="n">advanceDate</span><span class="p">(</span><span class="n">end_date</span><span class="p">,</span>  <span class="n">Period</span><span class="p">(</span><span class="s">'-%dB'</span> <span class="o">%</span> <span class="n">span</span><span class="p">))</span>
        <span class="n">bgn_date</span> <span class="o">=</span> <span class="n">bgn_date</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">)</span>
        
        <span class="c1"># 取HS300的span日前后的收盘价，并计算收益率：
</span>        <span class="n">bgn_price_HS300</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktIdxdGet</span><span class="p">(</span><span class="n">tradeDate</span><span class="o">=</span><span class="n">bgn_date</span><span class="p">,</span> <span class="n">ticker</span><span class="o">=</span><span class="s">'000300'</span><span class="p">,</span><span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'closeIndex'</span><span class="p">],</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">)</span>
        <span class="n">end_price_HS300</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktIdxdGet</span><span class="p">(</span><span class="n">tradeDate</span><span class="o">=</span><span class="n">end_date</span><span class="p">,</span> <span class="n">ticker</span><span class="o">=</span><span class="s">'000300'</span><span class="p">,</span><span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'closeIndex'</span><span class="p">],</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">)</span>
        <span class="n">rtn_HS300</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_price_HS300</span> <span class="o">-</span> <span class="n">bgn_price_HS300</span><span class="p">)</span> <span class="o">/</span> <span class="n">bgn_price_HS300</span>
        <span class="c1"># 提取HS300收益的数值
</span>        <span class="n">rtn_HS300</span> <span class="o">=</span> <span class="n">rtn_HS300</span><span class="p">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># 取各股票的span日前后的收盘价，并计算收益率：
</span>        <span class="n">bgn_price_stk</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktEqudGet</span><span class="p">(</span><span class="n">tradeDate</span><span class="o">=</span><span class="n">bgn_date</span><span class="p">,</span> <span class="n">secID</span><span class="o">=</span><span class="n">set_universe</span><span class="p">(</span><span class="s">'HS300'</span><span class="p">,</span> <span class="n">bgn_date</span><span class="p">),</span> <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'secID'</span><span class="p">,</span><span class="s">'closePrice'</span><span class="p">],</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">)</span>
        <span class="n">bgn_price_stk</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'secID'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">end_price_stk</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktEqudGet</span><span class="p">(</span><span class="n">tradeDate</span><span class="o">=</span><span class="n">end_date</span><span class="p">,</span> <span class="n">secID</span><span class="o">=</span><span class="n">set_universe</span><span class="p">(</span><span class="s">'HS300'</span><span class="p">,</span> <span class="n">bgn_date</span><span class="p">),</span> <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'secID'</span><span class="p">,</span><span class="s">'closePrice'</span><span class="p">],</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">)</span>
        <span class="n">end_price_stk</span><span class="p">.</span><span class="n">set_index</span><span class="p">(</span><span class="s">'secID'</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># 计算股票 超额收益率： 减去HS300收益
</span>        <span class="n">alpha_rtn_stk</span> <span class="o">=</span> <span class="p">(</span><span class="n">end_price_stk</span> <span class="o">-</span> <span class="n">bgn_price_stk</span><span class="p">)</span> <span class="o">/</span> <span class="n">bgn_price_stk</span> <span class="o">-</span> <span class="n">rtn_HS300</span>
        <span class="c1"># 将原来的closePrice列名改为label 
</span>        <span class="n">alpha_rtn_stk</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'label'</span><span class="p">]</span>
        <span class="c1"># 其中超额收益大于0的label取值为1，否则取0
</span>        <span class="n">alpha_rtn_stk</span> <span class="o">=</span> <span class="n">alpha_rtn_stk</span><span class="p">.</span><span class="n">applymap</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">x</span><span class="o">&gt;</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 丢弃缺失值
</span>        <span class="n">alpha_rtn_stk</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="c1"># 取 span 日前（即，bgn_data当日因子）股票的所有因子值：
</span>        <span class="n">stk_list</span> <span class="o">=</span> <span class="n">alpha_rtn_stk</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>  <span class="c1"># 股票列表：stk_list
</span>        <span class="n">factors_df</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktStockFactorsOneDayGet</span><span class="p">(</span><span class="n">tradeDate</span><span class="o">=</span><span class="n">bgn_date</span><span class="p">,</span> <span class="n">secID</span><span class="o">=</span><span class="n">stk_list</span><span class="p">,</span> <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'secID'</span><span class="p">]</span><span class="o">+</span><span class="n">features</span><span class="p">,</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">).</span><span class="n">set_index</span><span class="p">(</span><span class="s">'secID'</span><span class="p">)</span>
        <span class="c1"># 取factors_df中的每一列，对其因子去极值，中性化，标准化
</span>        <span class="k">for</span> <span class="n">factor</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
            <span class="n">raw_data</span> <span class="o">=</span> <span class="n">factors_df</span><span class="p">[</span><span class="n">factor</span><span class="p">].</span><span class="n">to_dict</span><span class="p">()</span>
            <span class="c1">#new_data = standardize(neutralize(winsorize(raw_data), bgn_date))
</span>            <span class="n">new_data</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">winsorize</span><span class="p">(</span><span class="n">raw_data</span><span class="p">))</span>
            <span class="n">alpha_rtn_stk</span><span class="p">[</span><span class="n">factor</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">new_data</span><span class="p">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">new_data</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="c1"># 重置索引，使secID重置为alpha_rtn_stk的列
</span>        <span class="n">alpha_rtn_stk</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># 删除'secID'这一列
</span>        <span class="n">alpha_rtn_stk</span><span class="p">.</span><span class="n">drop</span><span class="p">([</span><span class="s">'secID'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># 丢弃缺失值
</span>        <span class="n">alpha_rtn_stk</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># 与存储训练数据的DataFrame-train_data 合并
</span>        <span class="n">train_data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train_data</span><span class="p">,</span> <span class="n">alpha_rtn_stk</span><span class="p">])</span>
    
    <span class="c1"># 训练数据 因子值和类别 分离：
</span>    <span class="n">train_X</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">features</span><span class="p">].</span><span class="n">values</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="s">'label'</span><span class="p">].</span><span class="n">values</span>
    
    
    <span class="c1"># 准备 测试数据test_df：上一交易日HS300股票的因子值
</span>    <span class="c1"># 计算 近20个交易日日期，后续用以提剔除ST股、流动性差、未上市或涨停的股票
</span>    <span class="n">endDate</span> <span class="o">=</span> <span class="n">account</span><span class="p">.</span><span class="n">previous_date</span>
    <span class="n">beginDate</span> <span class="o">=</span> <span class="n">Calendar</span><span class="p">(</span><span class="s">'China.SSE'</span><span class="p">).</span><span class="n">advanceDate</span><span class="p">(</span><span class="n">endDate</span><span class="p">,</span> <span class="n">Period</span><span class="p">(</span><span class="s">'-20B'</span><span class="p">))</span>
    <span class="n">endDate</span> <span class="o">=</span> <span class="n">endDate</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y%m%d'</span><span class="p">)</span>
    <span class="n">beginDate</span> <span class="o">=</span> <span class="n">beginDate</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y%m%d'</span><span class="p">)</span>
    
    <span class="c1"># 剔除ST类股票
</span>    <span class="n">STlist</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">SecSTGet</span><span class="p">(</span><span class="n">secID</span><span class="o">=</span><span class="n">set_universe</span><span class="p">(</span><span class="s">'HS300'</span><span class="p">,</span> <span class="n">endDate</span><span class="p">),</span> <span class="n">beginDate</span><span class="o">=</span><span class="n">endDate</span><span class="p">,</span> <span class="n">endDate</span><span class="o">=</span><span class="n">endDate</span><span class="p">,</span> <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'secID'</span><span class="p">],</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">)[</span><span class="s">'secID'</span><span class="p">].</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">account</span><span class="p">.</span><span class="n">my_universe</span> <span class="o">=</span> <span class="p">[</span><span class="n">stk</span> <span class="k">for</span> <span class="n">stk</span> <span class="ow">in</span> <span class="n">account</span><span class="p">.</span><span class="n">universe</span> <span class="k">if</span> <span class="n">stk</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">STlist</span><span class="p">]</span>
    
    <span class="c1"># 去除流动性差的股票
</span>    <span class="n">turnOver</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktEqudGet</span><span class="p">(</span><span class="n">secID</span><span class="o">=</span><span class="n">account</span><span class="p">.</span><span class="n">my_universe</span><span class="p">,</span> <span class="n">beginDate</span><span class="o">=</span><span class="n">beginDate</span><span class="p">,</span> <span class="n">endDate</span><span class="o">=</span><span class="n">endDate</span><span class="p">,</span> <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'secID'</span><span class="p">,</span> <span class="s">'turnoverValue'</span><span class="p">],</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">)</span>
    <span class="c1"># 按secID分组，计算前20个交易日的平均成交量
</span>    <span class="n">turnOver</span> <span class="o">=</span> <span class="n">turnOver</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s">'secID'</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">turnOver</span> <span class="o">=</span> <span class="n">turnOver</span><span class="p">[</span><span class="n">turnOver</span><span class="p">[</span><span class="s">'turnoverValue'</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">10000000.</span><span class="p">].</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="n">account</span><span class="p">.</span><span class="n">my_universe</span> <span class="o">=</span> <span class="p">[</span><span class="n">stk</span> <span class="k">for</span> <span class="n">stk</span> <span class="ow">in</span> <span class="n">account</span><span class="p">.</span><span class="n">my_universe</span> <span class="k">if</span> <span class="n">stk</span> <span class="ow">in</span> <span class="n">turnOver</span><span class="p">[</span><span class="s">'secID'</span><span class="p">].</span><span class="n">tolist</span><span class="p">()]</span>  
    
    <span class="c1"># 去除新上市或复牌的股票
</span>    <span class="c1"># 获取昨日股票池里股票的开盘价
</span>    <span class="n">openPrice</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktEqudGet</span><span class="p">(</span><span class="n">tradeDate</span><span class="o">=</span><span class="n">endDate</span><span class="p">,</span> <span class="n">secID</span><span class="o">=</span><span class="n">account</span><span class="p">.</span><span class="n">my_universe</span><span class="p">,</span> <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'secID'</span><span class="p">,</span> <span class="s">'openPrice'</span><span class="p">],</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">)</span>
    <span class="n">openPrice</span> <span class="o">=</span> <span class="n">openPrice</span><span class="p">[</span><span class="n">openPrice</span><span class="p">[</span><span class="s">'openPrice'</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">openPrice</span> <span class="o">=</span> <span class="n">openPrice</span><span class="p">[</span><span class="n">openPrice</span><span class="p">[</span><span class="s">'openPrice'</span><span class="p">]</span> <span class="o">!=</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">]</span>
    <span class="n">account</span><span class="p">.</span><span class="n">my_universe</span> <span class="o">=</span> <span class="p">[</span><span class="n">stk</span> <span class="k">for</span> <span class="n">stk</span> <span class="ow">in</span> <span class="n">account</span><span class="p">.</span><span class="n">my_universe</span> <span class="k">if</span> <span class="n">stk</span> <span class="ow">in</span> <span class="n">openPrice</span><span class="p">[</span><span class="s">'secID'</span><span class="p">].</span><span class="n">tolist</span><span class="p">()]</span>
    
    <span class="n">test_df</span> <span class="o">=</span> <span class="n">DataAPI</span><span class="p">.</span><span class="n">MktStockFactorsOneDayGet</span><span class="p">(</span><span class="n">tradeDate</span><span class="o">=</span><span class="n">account</span><span class="p">.</span><span class="n">previous_date</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">"%Y%m%d"</span><span class="p">),</span> <span class="n">secID</span><span class="o">=</span><span class="n">account</span><span class="p">.</span><span class="n">my_universe</span><span class="p">,</span> <span class="n">field</span><span class="o">=</span><span class="p">[</span><span class="s">'secID'</span><span class="p">]</span><span class="o">+</span><span class="n">features</span><span class="p">,</span> <span class="n">pandas</span><span class="o">=</span><span class="s">"1"</span><span class="p">).</span><span class="n">set_index</span><span class="p">(</span><span class="s">'secID'</span><span class="p">)</span>
    <span class="c1"># 测试数据test_df中的每一列，对其因子去极值，中性化，标准化
</span>    <span class="k">for</span> <span class="n">factor</span> <span class="ow">in</span> <span class="n">features</span><span class="p">:</span>
        <span class="n">raw_data</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="n">factor</span><span class="p">].</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="c1">#new_data = standardize(neutralize(winsorize(raw_data), account.previous_date.strftime("%Y%m%d")))
</span>        <span class="n">new_data</span> <span class="o">=</span> <span class="n">standardize</span><span class="p">(</span><span class="n">winsorize</span><span class="p">(</span><span class="n">raw_data</span><span class="p">))</span>
        <span class="n">test_df</span><span class="p">[</span><span class="n">factor</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">new_data</span><span class="p">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">index</span><span class="o">=</span><span class="n">new_data</span><span class="p">.</span><span class="n">keys</span><span class="p">())</span>

    <span class="c1"># 丢弃缺失值：
</span>    <span class="n">test_df</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># 测试数据：
</span>    <span class="n">test_X</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
    <span class="c1"># 股票列表：
</span>    <span class="n">buy_list</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
    
    
    <span class="c1">####################################### 训练Logistic Regression模型 ##########################################
</span>    <span class="n">lr</span>  <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">)</span> <span class="c1"># C为正则化强度的系数，C越小 模型系数容忍值越大
</span>    <span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">predicted_proba</span> <span class="o">=</span> <span class="n">lr</span><span class="p">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
    <span class="c1"># 创建DataFrame：data为预测概率，index为对应的secID
</span>        <span class="c1"># 其中predicted_proba有若干个list，每一个list中有两个值，第一个划分为0类的概率，第二个为1类的概率
</span>    <span class="n">proba_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">predicted_proba</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">buy_list</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'probability'</span><span class="p">])</span>
    <span class="c1"># 对概率降序排列，取概率最大的前 stock 只 股票
</span>    <span class="n">proba_df</span> <span class="o">=</span> <span class="n">proba_df</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s">'probability'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)[:</span> <span class="n">stocksNumber</span><span class="p">]</span>
    
    <span class="c1"># 股票列表：buy_list
</span>    <span class="n">buy_list</span> <span class="o">=</span> <span class="n">proba_df</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">tolist</span><span class="p">()</span>
    
    <span class="c1"># 不在buy_list中的股票先全部卖出
</span>    <span class="k">for</span> <span class="n">stk</span> <span class="ow">in</span> <span class="n">account</span><span class="p">.</span><span class="n">valid_secpos</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">stk</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">buy_list</span><span class="p">:</span>
            <span class="n">order_to</span><span class="p">(</span><span class="n">stk</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># 字典change 记录buy_list中股票的仓位变化
</span>    <span class="n">change</span> <span class="o">=</span> <span class="p">{}</span>
    
    <span class="n">perCapital</span> <span class="o">=</span>  <span class="n">account</span><span class="p">.</span><span class="n">referencePortfolioValue</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">buy_list</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">stk</span> <span class="ow">in</span> <span class="n">buy_list</span><span class="p">:</span>
        <span class="c1"># 停牌或是还没有上市等原因不能交易
</span>        <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">account</span><span class="p">.</span><span class="n">referencePrice</span><span class="p">[</span><span class="n">stk</span><span class="p">])</span> <span class="ow">or</span> <span class="n">account</span><span class="p">.</span><span class="n">referencePrice</span><span class="p">[</span><span class="n">stk</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">change</span><span class="p">[</span><span class="n">stk</span><span class="p">]</span> <span class="o">=</span> <span class="n">perCapital</span><span class="o">/</span><span class="n">account</span><span class="p">.</span><span class="n">referencePrice</span><span class="p">[</span><span class="n">stk</span><span class="p">]</span> <span class="o">-</span> <span class="n">account</span><span class="p">.</span><span class="n">valid_secpos</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">stk</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># 根据仓位变化大小生序排列，即先卖后买            
</span>    <span class="k">for</span> <span class="n">stk</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">change</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">change</span><span class="p">.</span><span class="n">get</span><span class="p">):</span>
        <span class="n">order</span><span class="p">(</span><span class="n">stk</span><span class="p">,</span> <span class="n">change</span><span class="p">[</span><span class="n">stk</span><span class="p">])</span>

</code></pre></div></div>

        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
            
        
            
            
        
            
            
                
                    
                
                    
                
                    
                
            
                
                    
                
                    
                
                    
                
            
        
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2016/11/08/Cpp-Memory-Management/">C/C++内存管理详解</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2017/07/29/Raspberry-Pi-install-and-config/">Raspberry Pi 安装与配置</a></p>
        
    </div>
</div>




    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    <!-- <li><a href="#similar_posts">Similar Posts123</a></li>
                    <li><a href="#comments">Comments</a></li> -->
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>


<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">
  <div class="wrapper">
    <p class="description"> 如果你感受到痛苦，那么，你还活着；如果你感受到他人的痛苦，那么，你才是人。 </p>
    <p class="contact">
      Contact me at: 
      <a href="https://github.com/sanlo" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>   <a href="mailto:940072028@qq.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>  
      <a href="https://www.linkedin.com/in/sanlozhang" title="LinkedIn"><i class="fa fa-linkedin" aria-hidden="true"></i></a> 
    </p>
    <p class="power">
      <span> Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>. </span>
    </p>
  </div>
</footer>

    <div class="back-to-top">
    <a href="#top" class="scroll">
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/scroll.min.js " charset="utf-8"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="module">
      import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@9/dist/mermaid.esm.min.mjs';
      mermaid.initialize({ startOnLoad: true });
    </script>
  </body>

</html>
